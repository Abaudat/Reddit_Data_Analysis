{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit comments LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not already have PRAW installed, uncomment and run the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\granb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords as stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import praw\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a Reddit instance. To get a client id and secret please follow the [doc](https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example#first-steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=\"\", client_secret=\"\", user_agent=\"PRAW:LDA:0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_comments(username):\n",
    "    me = reddit.redditor(username)\n",
    "    l = list(me.comments.new(limit=None))\n",
    "    bodies = []\n",
    "    MIN_LENGTH = 50\n",
    "    for comment in l:\n",
    "        if len(comment.body) > MIN_LENGTH:\n",
    "            bodies.append(comment.body)\n",
    "    print(\"Number of comments: {}.\\nNumber of valid comments (length > {}): {}\\n\".format(len(l), MIN_LENGTH, len(bodies)))\n",
    "    vectorizer = CountVectorizer(strip_accents='unicode', stop_words=stopwords, analyzer='word')\n",
    "    data = vectorizer.fit_transform(bodies)\n",
    "    lda = LatentDirichletAllocation(n_components=7, doc_topic_prior=1.05, topic_word_prior=1.75, random_state=42)\n",
    "    lda = lda.fit(data)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for index, topic in enumerate(lda.components_):\n",
    "        best_words = np.argsort(topic)[::-1][:10]\n",
    "        word_agg = \"\"\n",
    "        for word in best_words:\n",
    "            word_agg += feature_names[word] + \", \"\n",
    "        print(\"Topic {}: {}\".format(index, word_agg[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments: 995.\n",
      "Number of valid comments (length > 50): 656\n",
      "\n",
      "Topic 0: side, clones, fix, blue, lane, would, tag, right, bush, better\n",
      "Topic 1: game, get, one, even, time, would, play, like, cards, make\n",
      "Topic 2: server, client, work, clients, send, problem, attack, port, master, one\n",
      "Topic 3: eth, node, price, book, board, stop, page, order, nodes, ground\n",
      "Topic 4: character, size, redditor, news, reading, element, please, title, stopped, posts\n",
      "Topic 5: fucking, little, shit, top, think, entire, usa, fuck, comment, know\n",
      "Topic 6: wyrm, conceal, spells, otk, summons, 118, call, auctioneer, mana, wild\n"
     ]
    }
   ],
   "source": [
    "LDA_comments('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
